{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Credit Card Approval\n",
    "\n",
    "\n",
    "Heba El-Shimy  \n",
    "IBM **Cloud** Developer Advocate\n",
    "\n",
    "\n",
    "<sub>GitHub: HebaNAS</sub>  \n",
    "<sub>Twitter: @heba_el_shimy</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, svm\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import PolynomialFeatures, LabelEncoder, StandardScaler\n",
    "import sklearn.feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Loading Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Insert your pandas DataFrame here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Checking that everything is correct\n",
    "pd.set_option('display.max_columns', 30)\n",
    "applicants.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Get some info about our Dataset and whether we have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# After running this cell we will see that we have no missing values\n",
    "applicants.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Convert columns with numbers as values but object as datatype into numeric\n",
    "cols = [1, 13]\n",
    "\n",
    "# Set error level to coerce so any string value will be replaced with NaN\n",
    "applicants[cols] = applicants[cols].apply(pd.to_numeric, errors='coerce')\n",
    "applicants.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Check if we have any NaN values\n",
    "applicants.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Handle missing values using scikit learn Imputer\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Define the values to replce and the strategy of choosing the replacement value\n",
    "imp = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n",
    "\n",
    "applicants[cols] = imp.fit_transform(applicants[cols])\n",
    "applicants.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Check if we have any NaN values\n",
    "applicants.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "applicants.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4. Descriptive analytics for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Describe columns with numerical values\n",
    "pd.set_option('precision', 3)\n",
    "applicants.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Find correlations\n",
    "applicants.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5. Visualize our Data to understand it better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Plot Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create Grid for pairwise relationships\n",
    "gr = sns.PairGrid(applicants, size=5, hue=15)\n",
    "gr = gr.map_diag(plt.hist)\n",
    "gr = gr.map_offdiag(plt.scatter)\n",
    "gr = gr.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Understand Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set up plot size\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# Attributes destribution\n",
    "a = sns.boxplot(orient=\"v\", palette=\"hls\", data=applicants.iloc[:, :13], fliersize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Tenure data distribution\n",
    "histogram = sns.distplot(applicants.iloc[:, 1], hist=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 6. Encode string values in data into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Use pandas get_dummies\n",
    "applicants_encoded = pd.get_dummies(applicants)\n",
    "applicants_encoded.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7. Create Training Set and Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create training data for non-preprocessed approach\n",
    "X_npp = applicants_encoded.iloc[:, :-2]\n",
    "pd.DataFrame(X_npp).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create training data for that will undergo preprocessing\n",
    "X = applicants_encoded.iloc[:, :-2]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split last column from original dataset as the labels column\n",
    "y = applicants[15]\n",
    "\n",
    "# Apply encoder to transform strings to numeric values 0 and 1\n",
    "le = LabelEncoder().fit(y)\n",
    "\n",
    "y_enc = le.transform(y)\n",
    "pd.DataFrame(y_enc).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 8. Detect outliers in numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Detect outlier using interquartile method and remove them\n",
    "def find_outliers(df):\n",
    "    quartile_1, quartile_3 = np.percentile(df, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "\n",
    "    outlier_indices = list(df.index[(df < lower_bound)|(df > upper_bound)])\n",
    "    outlier_values = list(df[outlier_indices])\n",
    "    \n",
    "    df[outlier_indices] = np.NaN\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find outliers in first column (continuous values)\n",
    "print(find_outliers(X[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find outliers in first column (continuous values)\n",
    "print(find_outliers(X[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find outliers in first column (continuous values)\n",
    "print(find_outliers(X[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find outliers in first column (continuous values)\n",
    "print(find_outliers(X[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find outliers in first column (continuous values)\n",
    "print(find_outliers(X[13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find outliers in first column (continuous values)\n",
    "print(find_outliers(X[14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "X.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the values to replce and the strategy of choosing the replacement value\n",
    "suspected_cols = [1, 2, 7, 10, 13, 14]\n",
    "imp = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n",
    "\n",
    "pd.DataFrame(X)[suspected_cols] = imp.fit_transform(pd.DataFrame(X)[suspected_cols])\n",
    "pd.DataFrame(X).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "pd.DataFrame(X).isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 9. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Select best features\n",
    "select = sklearn.feature_selection.SelectKBest(k=20)\n",
    "selected_features = select.fit(X, y_enc)\n",
    "indexes = selected_features.get_support(indices=True)\n",
    "col_names_selected = [pd.DataFrame(X).columns[i] for i in indexes]\n",
    "\n",
    "X_selected = pd.DataFrame(X)[col_names_selected]\n",
    "pd.DataFrame(X_selected).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 10. Split our dataset into train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Split non-preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_npp, X_test_npp, y_train_npp, y_test_npp = train_test_split(X_npp, y_enc,\\\n",
    "                                                    test_size=0.3, random_state=42)\n",
    "print(X_train_npp.shape, y_train_npp.shape)\n",
    "print(X_test_npp.shape, y_test_npp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_enc,\\\n",
    "                                                    test_size=0.3, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 11. Scale our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use StandardScaler\n",
    "scaler = preprocessing.StandardScaler().fit(X_train, y_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "pd.DataFrame(X_train_scaled, columns=pd.DataFrame(X_train).columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 12. Start building a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Logestic Regression on non-preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_lr_npp = LogisticRegression()\n",
    "clf_lr_npp.fit(X_train_npp, y_train_npp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Logestic Regression on preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_lr = LogisticRegression()\n",
    "model = clf_lr.fit(X_train_scaled, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 13. Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use the scaler fit on trained data to scale our test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "pd.DataFrame(X_test_scaled, columns=pd.DataFrame(X_train).columns).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluate Logistic Regression on non-preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_score_lr_npp = clf_lr_npp.decision_function(X_test_npp)\n",
    "y_score_lr_npp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Get accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_lr_npp = clf_lr_npp.predict(X_test_npp)\n",
    "acc_lr_npp = accuracy_score(y_test_npp, y_pred_lr_npp)\n",
    "print(acc_lr_npp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Get Precision vs. Recall score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "average_precision_lr_npp = average_precision_score(y_test_npp, y_score_lr_npp)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision_lr_npp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluate Logistic Regression on preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_score_lr = clf_lr.decision_function(X_test_scaled)\n",
    "y_score_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_lr = clf_lr.predict(X_test_scaled)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(acc_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "average_precision_lr = average_precision_score(y_test, y_score_lr)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 14. ROC Curve and models comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot SVC ROC Curve\n",
    "plt.figure(0, figsize=(15,10)).clf()\n",
    "\n",
    "fpr_lr_npp, tpr_lr_npp, thresh_lr_npp = metrics.roc_curve(y_test_npp, y_score_lr_npp)\n",
    "auc_lr_npp = metrics.roc_auc_score(y_test_npp, y_score_lr_npp)\n",
    "plt.plot(fpr_lr_npp, tpr_lr_npp, label=\"Logistic Regression on Non-preprocessed Data, auc=\" + str(auc_lr_npp))\n",
    "\n",
    "fpr_lr, tpr_lr, thresh_lr = metrics.roc_curve(y_test, y_score_lr)\n",
    "auc_lr = metrics.roc_auc_score(y_test, y_score_lr)\n",
    "plt.plot(fpr_lr, tpr_lr, label=\"Logistic Regression on Preprocessed Data, auc=\" + str(auc_lr))\n",
    "\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('False Positives')\n",
    "plt.ylabel('True Positives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bonus: Deploy model on the cloud using IBM Watson Machine Learning\n",
    "\n",
    "We have our model, but we want to use it through multiple apps. A solution is to deploy it on the cloud as an endpoint (url) and send data collected from a web/mobile app as a REST API call with data sent in the form of a JSON request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your credentials here\n",
    "\n",
    "wml_credentials = {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "client = WatsonMachineLearningAPIClient( wml_credentials )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wml_credentials['apikey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url     = \"https://iam.bluemix.net/oidc/token\"\n",
    "headers = { \"Content-Type\" : \"application/x-www-form-urlencoded\" }\n",
    "data    = \"apikey=\" + wml_credentials['apikey'] + \"&grant_type=urn:ibm:params:oauth:grant-type:apikey\"\n",
    "IBM_cloud_IAM_uid = \"bx\"\n",
    "IBM_cloud_IAM_pwd = \"bx\"\n",
    "response  = requests.post( url, headers=headers, data=data, auth=( IBM_cloud_IAM_uid, IBM_cloud_IAM_pwd ) )\n",
    "iam_token = response.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model\n",
    "\n",
    "model_props = {client.repository.ModelMetaNames.AUTHOR_NAME: \"Your name\", \n",
    "               client.repository.ModelMetaNames.NAME: \"Credit Card Approval Model\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish model in Watson Machine Learning repository on Cloud\n",
    "published_model = client.repository.store_model(model=model, meta_props=model_props, \\\n",
    "                                                training_data=X_train_scaled, training_target=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model deployment\n",
    "\n",
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "created_deployment = client.deployments.create(published_model_uid, \"Deployment of Credit Card Approval Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Scoring URL\n",
    "scoring_endpoint = client.deployments.get_scoring_url(created_deployment)\n",
    "\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get model details and expected input\n",
    "model_details = client.repository.get_details(published_model_uid)\n",
    "print(json.dumps(model_details, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending data to the model\n",
    "Sending new data (may be collected from web/mobile app) in the format the model is excpecting as shown above.\n",
    "We get back a response with the predicted class (0 - Credit Card Application will be rejected)\n",
    "and probabilities of both classes (0 or Application Rejection has a probability of 1 which is very high, 1 or Application Acceptance has a probability of 5.096701256722081e-98 which is very low. This gives us an idea about the model's confidence of its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![postman](https://github.com/HebaNAS/IBM-Watson-Studio-Enablement/blob/master/02-CreditCardApprovalModel/imgs/API-Call.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References:\n",
    "\n",
    "#### <a name=\"first\" id=\"first\"></a><sub>[1] https://www.sciencedirect.com/science/article/abs/pii/S0148296318301231 \"Customer churn prediction in telecommunication industry using data certainty\"</sub>  \n",
    "#### <a name=\"second\" id=\"second\"></a><sub>[2] https://www.signal.co/blog/understanding-customer-churn/ \"10 Stats Expose the Real Connection Between Customer Experience and Customer Churn\"</sub>  \n",
    "#### <a name=\"third\" id=\"third\"></a><sub>[3] https://www.pinterest.com/pin/456904324667676431/ \"Mobile Telco Churn Infographic\"</sub>  \n",
    "#### <sub>[4] https://pandas.pydata.org/pandas-docs/stable/ \"Pandas Documentation\"</sub>  \n",
    "#### <sub>[5] http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html \"Scikit-Learn Imputer\"</sub>  \n",
    "#### <sub>[6] https://github.com/ibm-watson-data-lab/pixiedust/wiki/Tutorial:-Extending-the-PixieDust-Visualization \"PixieDust Documentation\"</sub>\n",
    "#### <sub>[7] https://seaborn.pydata.org/ \"Seaborn Documentation\"</sub>\n",
    "#### <sub>[8] http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder \"Scikit-Learn LabelEncoder\"</sub>\n",
    "#### <sub>[9] http://colingorrie.github.io/outlier-detection.html \"Outlier Detection Methods\"</sub>\n",
    "#### <sub>[10] http://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html#sphx-glr-auto-examples-linear-model-plot-polynomial-interpolation-py \"Scikit-Learn Polynomial\"</sub>\n",
    "#### <sub>[11] http://scikit-learn.org/stable/modules/feature_selection.html \"Scikit-Learn Feature Selection\"</sub>\n",
    "#### <sub>[12] http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler \"Scikit-Learn StandardScaler\"</sub>\n",
    "#### <sub>[13] http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC \"Scikit-Learn SVC\"</sub>\n",
    "#### <sub>[14] http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression \"Scikit-Learn Logistic Regression\"</sub>\n",
    "#### <sub>[15] http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \"Scikit-Learn MLP Classifier\"</sub>\n",
    "#### <sub>[16] http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score \"Scikit-Learn Accuracy Score\"</sub>\n",
    "#### <sub>[17] http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score \"Scikit-Learn Average Precision Score\"</sub>\n",
    "#### <sub>[18] https://www.sciencedirect.com/science/article/pii/S016786550500303X \"An introduction to ROC analysis\"</sub>\n",
    "#### <sub>[19] https://wml-api-pyclient.mybluemix.net/ \"Watson Machine Learning Client Documentation\"</sub>\n",
    "#### <sub>[20] https://dataplatform.ibm.com/docs/content/analyze-data/ml-deploy-notebook.html?context=analytics \"IBM Watson Studio Documentation-Deploy a model from a notebook\"</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
